# llm-graph-tree-vqa

How well does **GPT-4V** perform **Visual Question Answering (VQA)** on **Data Structures**?

There is no dataset for **VQA** on graph and tree data structures in previous work, so we must create one. We create a standard, repeatable process for selecting and obtaining **VQA** tasks that must fall under a certain criteria.

### Workflow
Technical overview of creation and evaluation of dataset.

<img width="1472" alt="lmm-graph-tree-vqa" src="https://github.com/gutbash/lmm-graph-tree-vqa/assets/44552816/828ceb7c-a682-4ab2-ad47-7ba972c7ab53">

### Model
We elect **GPT-4V** as the primary model to observe for this evaluation.

<img width="1087" alt="model" src="https://github.com/gutbash/lmm-graph-tree-vqa/assets/44552816/9fa31867-6af8-4db7-858c-21efc3e3e199">

### Dataset
Overview of the dataset architecture.

<img width="499" alt="dataset analysis" src="https://github.com/gutbash/lmm-graph-tree-vqa/assets/44552816/df2150f8-a86c-4f14-bd5d-a42ff58b7d0a">
