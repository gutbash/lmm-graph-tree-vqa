# lmm-graph-tree-vqa

How well does **GPT-4V** perform **Visual Question Answering (VQA)** on **Data Structures**?

There is no dataset for **VQA** on graph and tree data structures in previous work, so we must create one. We create a standard, repeatable process for selecting and obtaining **VQA** tasks that must fall under a certain criteria.

### Workflow
Technical overview of creation and evaluation of dataset.

<img width="1428" alt="lmm-graph-tree-vqa (2)" src="https://github.com/gutbash/lmm-graph-tree-vqa/assets/44552816/b5a319b2-c398-4364-a347-85ca7b4f04f5">

### Model
We elect **GPT-4V** as the primary model to observe for this evaluation.

<img width="1087" alt="model" src="https://github.com/gutbash/lmm-graph-tree-vqa/assets/44552816/9fa31867-6af8-4db7-858c-21efc3e3e199">

### Dataset
Overview of the dataset architecture.

<img width="1628" alt="dataset analysis" src="https://github.com/gutbash/lmm-graph-tree-vqa/assets/44552816/633283e3-302b-40d0-9747-e207d906097d">

### Task
Example of task generations.

<img width="1767" alt="lmm-graph-tree-vqa (1)" src="https://github.com/gutbash/lmm-graph-tree-vqa/assets/44552816/4de95419-f6be-4d8f-b4ae-6ca3d24b1a1b">
